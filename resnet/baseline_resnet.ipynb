{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load resnet18\n",
      "Start training\n",
      "Epoch 1, Loss: 1.410898466904958  |  Train Accuracy: 35.67%  |  Test Accuracy: 32.22%\n",
      "Epoch 2, Loss: 1.3350616892178853  |  Train Accuracy: 38.76%  |  Test Accuracy: 33.33%\n",
      "Epoch 3, Loss: 1.1826454997062683  |  Train Accuracy: 52.53%  |  Test Accuracy: 50.00%\n",
      "Epoch 4, Loss: 1.034067173798879  |  Train Accuracy: 52.81%  |  Test Accuracy: 48.89%\n",
      "Epoch 5, Loss: 0.8782643576463064  |  Train Accuracy: 64.89%  |  Test Accuracy: 52.22%\n",
      "Epoch 6, Loss: 0.7477518518765768  |  Train Accuracy: 76.12%  |  Test Accuracy: 57.78%\n",
      "Epoch 7, Loss: 0.6276103655497233  |  Train Accuracy: 77.53%  |  Test Accuracy: 62.22%\n",
      "Epoch 8, Loss: 0.5431061883767446  |  Train Accuracy: 85.96%  |  Test Accuracy: 61.11%\n",
      "Epoch 9, Loss: 0.4514639874299367  |  Train Accuracy: 94.94%  |  Test Accuracy: 63.33%\n",
      "Epoch 10, Loss: 0.3901234418153763  |  Train Accuracy: 96.07%  |  Test Accuracy: 63.33%\n",
      "Epoch 11, Loss: 0.3103527973095576  |  Train Accuracy: 99.72%  |  Test Accuracy: 57.78%\n",
      "Epoch 12, Loss: 0.25892799844344455  |  Train Accuracy: 98.88%  |  Test Accuracy: 61.11%\n",
      "Epoch 13, Loss: 0.2349717691540718  |  Train Accuracy: 100.00%  |  Test Accuracy: 56.67%\n",
      "Epoch 14, Loss: 0.18863513072331747  |  Train Accuracy: 100.00%  |  Test Accuracy: 61.11%\n",
      "Epoch 15, Loss: 0.1430175540347894  |  Train Accuracy: 100.00%  |  Test Accuracy: 58.89%\n",
      "Epoch 16, Loss: 0.12468308831254642  |  Train Accuracy: 100.00%  |  Test Accuracy: 55.56%\n",
      "Epoch 17, Loss: 0.10625416661302249  |  Train Accuracy: 100.00%  |  Test Accuracy: 58.89%\n",
      "Epoch 18, Loss: 0.09228391821185748  |  Train Accuracy: 100.00%  |  Test Accuracy: 61.11%\n",
      "Epoch 19, Loss: 0.07419226815303166  |  Train Accuracy: 100.00%  |  Test Accuracy: 60.00%\n",
      "Epoch 20, Loss: 0.06732044244805972  |  Train Accuracy: 100.00%  |  Test Accuracy: 55.56%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((230, 230)),  # Resize the image\n",
    "#     transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Color jitter\n",
    "#     transforms.RandomCrop(224),  # Random crop to 224x224\n",
    "#     transforms.ToTensor(),  # Convert to tensor\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "# ])\n",
    "\n",
    "\n",
    "print(\"Load resnet18\")\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "# model = models.resnet18(pretrained=True)\n",
    "num_classes = 4  # Number of categories\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001) # not as good\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Specify the dataset directory\n",
    "data_dir = '/mnt/c/Users/Sean/Downloads/OCT_DATASET_DO_NOT_SHARE_WITH_ANYONE/labeled/'\n",
    "\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "train_size = 0.8  # 80% for training\n",
    "test_size = 1 - train_size\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=test_size, random_state=69)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(\"Start training\")\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}', end='  |  ')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy:.2f}%', end='  |  ')\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "torch.save(model.state_dict(), 'resnet18_e20.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradcam code\n",
    "\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class _BaseWrapper(object):\n",
    "    def __init__(self, model):\n",
    "        super(_BaseWrapper, self).__init__()\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.model = model\n",
    "        self.handlers = []  # a set of hook function handlers\n",
    "\n",
    "    def _encode_one_hot(self, ids):\n",
    "        one_hot = torch.zeros_like(self.logits).to(self.device)\n",
    "        one_hot.scatter_(1, ids, 1.0)\n",
    "        return one_hot\n",
    "\n",
    "    def forward(self, image):\n",
    "        self.image_shape = image.shape[2:]\n",
    "        self.logits = self.model(image)\n",
    "        self.probs = F.softmax(self.logits, dim=1)\n",
    "        return self.probs.sort(dim=1, descending=True)  # ordered results\n",
    "\n",
    "    def backward(self, ids):\n",
    "        \"\"\"\n",
    "        Class-specific backpropagation\n",
    "        \"\"\"\n",
    "        one_hot = self._encode_one_hot(ids)\n",
    "        self.model.zero_grad()\n",
    "        self.logits.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "    def generate(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def remove_hook(self):\n",
    "        \"\"\"\n",
    "        Remove all the forward/backward hook functions\n",
    "        \"\"\"\n",
    "        for handle in self.handlers:\n",
    "            handle.remove()\n",
    "\n",
    "\n",
    "class BackPropagation(_BaseWrapper):\n",
    "    def forward(self, image):\n",
    "        self.image = image.requires_grad_()\n",
    "        return super(BackPropagation, self).forward(self.image)\n",
    "\n",
    "    def generate(self):\n",
    "        gradient = self.image.grad.clone()\n",
    "        self.image.grad.zero_()\n",
    "        return gradient\n",
    "\n",
    "\n",
    "class GuidedBackPropagation(BackPropagation):\n",
    "    \"\"\"\n",
    "    \"Striving for Simplicity: the All Convolutional Net\"\n",
    "    https://arxiv.org/pdf/1412.6806.pdf\n",
    "    Look at Figure 1 on page 8.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(GuidedBackPropagation, self).__init__(model)\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            # Cut off negative gradients\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                return (F.relu(grad_in[0]),)\n",
    "\n",
    "        for module in self.model.named_modules():\n",
    "            self.handlers.append(module[1].register_backward_hook(backward_hook))\n",
    "\n",
    "\n",
    "class Deconvnet(BackPropagation):\n",
    "    \"\"\"\n",
    "    \"Striving for Simplicity: the All Convolutional Net\"\n",
    "    https://arxiv.org/pdf/1412.6806.pdf\n",
    "    Look at Figure 1 on page 8.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(Deconvnet, self).__init__(model)\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            # Cut off negative gradients and ignore ReLU\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                return (F.relu(grad_out[0]),)\n",
    "\n",
    "        for module in self.model.named_modules():\n",
    "            self.handlers.append(module[1].register_backward_hook(backward_hook))\n",
    "\n",
    "\n",
    "class GradCAM(_BaseWrapper):\n",
    "    \"\"\"\n",
    "    \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\"\n",
    "    https://arxiv.org/pdf/1610.02391.pdf\n",
    "    Look at Figure 2 on page 4\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, candidate_layers=None):\n",
    "        super(GradCAM, self).__init__(model)\n",
    "        self.fmap_pool = {}\n",
    "        self.grad_pool = {}\n",
    "        self.candidate_layers = candidate_layers  # list\n",
    "\n",
    "        def save_fmaps(key):\n",
    "            def forward_hook(module, input, output):\n",
    "                self.fmap_pool[key] = output.detach()\n",
    "\n",
    "            return forward_hook\n",
    "\n",
    "        def save_grads(key):\n",
    "            def backward_hook(module, grad_in, grad_out):\n",
    "                self.grad_pool[key] = grad_out[0].detach()\n",
    "\n",
    "            return backward_hook\n",
    "\n",
    "        # If any candidates are not specified, the hook is registered to all the layers.\n",
    "        for name, module in self.model.named_modules():\n",
    "            if self.candidate_layers is None or name in self.candidate_layers:\n",
    "                self.handlers.append(module.register_forward_hook(save_fmaps(name)))\n",
    "                self.handlers.append(module.register_backward_hook(save_grads(name)))\n",
    "\n",
    "    def _find(self, pool, target_layer):\n",
    "        if target_layer in pool.keys():\n",
    "            return pool[target_layer]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layer name: {}\".format(target_layer))\n",
    "\n",
    "    def generate(self, target_layer):\n",
    "        fmaps = self._find(self.fmap_pool, target_layer)\n",
    "        grads = self._find(self.grad_pool, target_layer)\n",
    "        weights = F.adaptive_avg_pool2d(grads, 1)\n",
    "\n",
    "        gcam = torch.mul(fmaps, weights).sum(dim=1, keepdim=True)\n",
    "        gcam = F.relu(gcam)\n",
    "        gcam = F.interpolate(\n",
    "            gcam, self.image_shape, mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "\n",
    "        B, C, H, W = gcam.shape\n",
    "        gcam = gcam.view(B, -1)\n",
    "        gcam -= gcam.min(dim=1, keepdim=True)[0]\n",
    "        gcam /= gcam.max(dim=1, keepdim=True)[0]\n",
    "        gcam = gcam.view(B, C, H, W)\n",
    "\n",
    "        return gcam\n",
    "\n",
    "\n",
    "def occlusion_sensitivity(\n",
    "    model, images, ids, mean=None, patch=35, stride=1, n_batches=128\n",
    "):\n",
    "    \"\"\"\n",
    "    \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\"\n",
    "    https://arxiv.org/pdf/1610.02391.pdf\n",
    "    Look at Figure A5 on page 17\n",
    "\n",
    "    Originally proposed in:\n",
    "    \"Visualizing and Understanding Convolutional Networks\"\n",
    "    https://arxiv.org/abs/1311.2901\n",
    "    \"\"\"\n",
    "\n",
    "    torch.set_grad_enabled(False)\n",
    "    model.eval()\n",
    "    mean = mean if mean else 0\n",
    "    patch_H, patch_W = patch if isinstance(patch, Sequence) else (patch, patch)\n",
    "    pad_H, pad_W = patch_H // 2, patch_W // 2\n",
    "\n",
    "    # Padded image\n",
    "    images = F.pad(images, (pad_W, pad_W, pad_H, pad_H), value=mean)\n",
    "    B, _, H, W = images.shape\n",
    "    new_H = (H - patch_H) // stride + 1\n",
    "    new_W = (W - patch_W) // stride + 1\n",
    "\n",
    "    # Prepare sampling grids\n",
    "    anchors = []\n",
    "    grid_h = 0\n",
    "    while grid_h <= H - patch_H:\n",
    "        grid_w = 0\n",
    "        while grid_w <= W - patch_W:\n",
    "            grid_w += stride\n",
    "            anchors.append((grid_h, grid_w))\n",
    "        grid_h += stride\n",
    "\n",
    "    # Baseline score without occlusion\n",
    "    baseline = model(images).detach().gather(1, ids)\n",
    "\n",
    "    # Compute per-pixel logits\n",
    "    scoremaps = []\n",
    "    for i in tqdm(range(0, len(anchors), n_batches), leave=False):\n",
    "        batch_images = []\n",
    "        batch_ids = []\n",
    "        for grid_h, grid_w in anchors[i : i + n_batches]:\n",
    "            images_ = images.clone()\n",
    "            images_[..., grid_h : grid_h + patch_H, grid_w : grid_w + patch_W] = mean\n",
    "            batch_images.append(images_)\n",
    "            batch_ids.append(ids)\n",
    "        batch_images = torch.cat(batch_images, dim=0)\n",
    "        batch_ids = torch.cat(batch_ids, dim=0)\n",
    "        scores = model(batch_images).detach().gather(1, batch_ids)\n",
    "        scoremaps += list(torch.split(scores, B))\n",
    "\n",
    "    diffmaps = torch.cat(scoremaps, dim=1) - baseline\n",
    "    diffmaps = diffmaps.view(B, new_H, new_W)\n",
    "\n",
    "    return diffmaps\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "def save_gradcam(filename, gcam, raw_image, paper_cmap=False):\n",
    "    gcam = gcam.cpu().numpy()\n",
    "    cmap = cm.jet_r(gcam)[..., :3] * 255.0\n",
    "    if paper_cmap:\n",
    "        alpha = gcam[..., None]\n",
    "        gcam = alpha * cmap + (1 - alpha) * raw_image\n",
    "    else:\n",
    "        gcam = (cmap.astype(float) + raw_image.astype(float)) / 2\n",
    "    cv2.imwrite(filename, np.uint8(gcam))\n",
    "\n",
    "def inverse_transform(image):\n",
    "    inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
    "    std=[1 / 0.229, 1 / 0.224, 1 / 0.225]\n",
    ")\n",
    "    denormalized_tensor = inv_normalize(image)\n",
    "\n",
    "    # 2. Convert the tensor to a NumPy array\n",
    "    numpy_image = denormalized_tensor.cpu().numpy()\n",
    "\n",
    "    # 3. Reshape the array and convert it to an OpenCV image\n",
    "    numpy_image = np.transpose(numpy_image, (1, 2, 0))  # Change tensor shape (C, H, W) to (H, W, C)\n",
    "    opencv_image = (numpy_image * 255).astype(np.uint8)  # Convert to 8-bit image\n",
    "\n",
    "    return opencv_image\n",
    "\n",
    "\n",
    "def demo2(image, output_dir, target_class):\n",
    "    \"\"\"\n",
    "    Generate Grad-CAM at different layers of ResNet-152\n",
    "    \"\"\"\n",
    "\n",
    "    classes = [0,1,2,3]\n",
    "\n",
    "    # Model\n",
    "    model = models.resnet152(pretrained=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # The four residual layers\n",
    "    target_layers = [\"relu\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"]\n",
    "\n",
    "    # convert to cv2 images\n",
    "    raw_images = [inverse_transform(x) for x in image]\n",
    "    images = torch.stack(image).to(device)\n",
    "\n",
    "    gcam = GradCAM(model=model)\n",
    "    probs, ids = gcam.forward(images)\n",
    "    ids_ = torch.LongTensor([[target_class]] * len(images)).to(device)\n",
    "    gcam.backward(ids=ids_)\n",
    "\n",
    "    for target_layer in target_layers:\n",
    "        print(\"Generating Grad-CAM @{}\".format(target_layer))\n",
    "\n",
    "        # Grad-CAM\n",
    "        regions = gcam.generate(target_layer=target_layer)\n",
    "\n",
    "        for j in range(len(images)):\n",
    "            print(\n",
    "                \"\\t#{}: {} ({:.5f})\".format(\n",
    "                    j, classes[target_class], float(probs[ids == target_class])\n",
    "                )\n",
    "            )\n",
    "\n",
    "            save_gradcam(\n",
    "                filename=os.path.join(\n",
    "                    output_dir,\n",
    "                    \"{}-{}-gradcam-{}-{}.png\".format(\n",
    "                        j, \"resnet\", target_layer, classes[target_class]\n",
    "                    ),\n",
    "                ),\n",
    "                gcam=regions[j, 0],\n",
    "                raw_image=raw_images[j],\n",
    "            )\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    if labels[0] != 0:\n",
    "        continue\n",
    "    label = labels[0]\n",
    "    image1 = images[0]\n",
    "    demo2([image1], \"/home/sean/Documents/ml/rp-ml/\", label)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
